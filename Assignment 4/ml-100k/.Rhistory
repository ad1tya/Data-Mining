plotcp(model)
pruneModel <- prune(model,cp=0.017)
rpart.plot(pruneModel, extra=104, fallen.leaves = T, type=4)
pred <- predict(pruneModel, test[, 1:11], type="class")
confusionMatrix(pred, test[, 11])
printcp(model)
plotcp(model)
rpart.plot(pruneModel, extra=104, fallen.leaves = T, type=4)
rpart.plot(model, extra=104, fallen.leaves = T, type=4)
rpart.plot(pruneModel, extra=104, fallen.leaves = T, type=4)
confusionMatrix(pred, test[, 11])
plotcp(model)
rpart.plot(model, extra=104, fallen.leaves = T, type=4)
pairs.panels(train[1:11])
model1 = rplot(label ~ age + sex + tb + db + aap +
sgpaa + alb + ag , method="class", data=train)
rpart.plot(model1, extra=104, fallen.leaves = T, type=4)
pred1 <- predict(model1, test[, 1:11], type="class")
confusionMatrix(pred1, test[, 11])
model1 = rplot(label ~ age + sex + tb + aap +
sgpaa + alb + ag , method="class", data=train)
library(rpart)
library(caret)
library(rpart.plot)
library(psych)
library(ROCR)
model1 = rplot(label ~ age + sex + tb + aap +
sgpaa + alb + ag , method="class", data=train)
model1 = rpart(label ~ age + sex + tb + aap +
sgpaa + alb + ag , method="class", data=train)
rpart.plot(model1, extra=104, fallen.leaves = T, type=4)
pred1 <- predict(model1, test[, 1:11], type="class")
confusionMatrix(pred1, test[, 11])
pairs.panels(train[1:11])
pred.rocrred <- predict(model, newdata=test, type="prob")[,2]
f.predred <- prediction(pred.rocrred, test$label)
f.perfred <- performance(f.predred, "tpr", "fpr")
plot(f.perfred, colorize=T, lwd=3)
abline(0,1)
auc <- performance(f.predred, measure = "auc")
auc@y.values[[1]]
pred.rocrred <- predict(model1, newdata=test, type="prob")[,2]
f.predred <- prediction(pred.rocrred, test$label)
f.perfred <- performance(f.predred, "tpr", "fpr")
plot(f.perfred, colorize=T, lwd=3)
abline(0,1)
auc <- performance(f.predred, measure = "auc")
auc@y.values[[1]]
pred.rocrred <- predict(pruneModel, newdata=test, type="prob")[,2]
f.predred <- prediction(pred.rocrred, test$label)
f.perfred <- performance(f.predred, "tpr", "fpr")
plot(f.perfred, colorize=T, lwd=3)
abline(0,1)
auc <- performance(f.predred, measure = "auc")
auc@y.values[[1]]
pred.rocrred <- predict(model1, newdata=test, type="prob")[,2]
f.predred <- prediction(pred.rocrred, test$label)
f.perfred <- performance(f.predred, "tpr", "fpr")
plot(f.perfred, colorize=T, lwd=3)
abline(0,1)
auc <- performance(f.predred, measure = "auc")
auc@y.values[[1]]
pred.rocrred <- predict(model, newdata=test, type="prob")[,2]
f.predred <- prediction(pred.rocrred, test$label)
f.perfred <- performance(f.predred, "tpr", "fpr")
plot(f.perfred, colorize=T, lwd=3)
abline(0,1)
auc <- performance(f.predred, measure = "auc")
auc@y.values[[1]]
pred1.rocrred <- predict(model1, newdata=test, type="prob")[,2]
f.predred <- prediction(pred1.rocrred, test$label)
f.perfred <- performance(f.predred, "tpr", "fpr")
plot(f.perfred, colorize=T, lwd=3)
abline(0,1)
auc <- performance(f.predred, measure = "auc")
auc@y.values[[1]]
prunePred.rocrred <- predict(pruneModel, newdata=test, type="prob")[,2]
f.predred <- prediction(prunePred.rocrred, test$label)
f.perfred <- performance(f.predred, "tpr", "fpr")
plot(f.perfred, colorize=T, lwd=3)
abline(0,1)
auc <- performance(f.predred, measure = "auc")
auc@y.values[[1]]
pred.rocrred <- predict(model, newdata=test, type="prob")[,2]
f.pred <- prediction(pred.rocrred, test$label)
f.perf <- performance(f.pred, "tpr", "fpr")
plot(f.perfred, colorize=T, lwd=3)
abline(0,1)
auc <- performance(f.predred, measure = "auc")
auc@y.values[[1]]
prunePred.rocrred <- predict(pruneModel, newdata=test, type="prob")[,2]
f.prunePred <- prediction(prunePred.rocrred, test$label)
f.perfPrune <- performance(f.prunePred, "tpr", "fpr")
plot(f.perfPrune, colorize=T, lwd=3)
abline(0,1)
auc <- performance(f.prunePred, measure = "auc")
auc@y.values[[1]]
pred.rocrred <- predict(model, newdata=test, type="prob")[,2]
f.pred <- prediction(pred.rocrred, test$label)
f.perf <- performance(f.pred, "tpr", "fpr")
plot(f.perf, colorize=T, lwd=3)
abline(0,1)
auc <- performance(f.pred, measure = "auc")
auc@y.values[[1]]
prunePred.rocrred <- predict(pruneModel, newdata=test, type="prob")[,2]
f.prunePred <- prediction(prunePred.rocrred, test$label)
f.perfPrune <- performance(f.prunePred, "tpr", "fpr")
plot(f.perfPrune, colorize=T, lwd=3)
abline(0,1)
auc <- performance(f.prunePred, measure = "auc")
auc@y.values[[1]]
pred1.rocrred <- predict(model1, newdata=test, type="prob")[,2]
f.pred1 <- prediction(pred1.rocrred, test$label)
f.perf1 <- performance(f.pred1, "tpr", "fpr")
plot(f.perf1, colorize=T, lwd=3)
abline(0,1)
auc <- performance(f.pred1, measure = "auc")
auc@y.values[[1]]
prunePred.rocrred <- predict(pruneModel, newdata=test, type="prob")[,2]
f.prunePred <- prediction(prunePred.rocrred, test$label)
f.perfPrune <- performance(f.prunePred, "tpr", "fpr")
plot(f.perfPrune, colorize=T, lwd=3)
abline(0,1)
auc <- performance(f.prunePred, measure = "auc")
auc@y.values[[1]]
pred.rocrred <- predict(model, newdata=test, type="prob")[,2]
f.pred <- prediction(pred.rocrred, test$label)
f.perf <- performance(f.pred, "tpr", "fpr")
plot(f.perf, colorize=T, lwd=3)
abline(0,1)
auc <- performance(f.pred, measure = "auc")
auc@y.values[[1]]
ilpdOrg <- read.csv("C:\\Users\\Pravin Kulkarni\\Desktop\\Indian Liver Patient Dataset (ILPD).csv")
unique(ilpdOrg)
new_DF <- ilpdOrg[is.na(ilpdOrg$Var),]
new_DF <- ilpdOrg[is.na(ilpdOrg$sgpaa),]
new_DF <- ilpdOrg[rowSums(is.na(ilpdOrg)) > 0,]
summary(new_DF)
View(new_DF)
unique (unlist (lapply (ilpdOrg, function (ilpdOrg) which (is.na (ilpdOrg)))))
library(mice)
install.packages(mice)
install.packages("mice")
library(mice)
md.pattern(ilpdOrg)
tempData <- mice(ilpdOrg,m=5,maxit=50,meth='pmm',seed=500)
summary(tempData)
imputedData <- complete(tempData,1)
summary(imputedData)
View(imputedData)
?read.scv
?read.csv
ilpd = read.csv("C:\\Users\\Pravin Kulkarni\\Desktop\\ILPD.csv")
ilpd = read.csv("C:\\Users\\Pravin Kulkarni\\Desktop\\ILPD.csv")
data <- ilpd[1:5,]
data
data <- ilpd[,1]
data
data <- ilpd[1:5,c(1:3)]
data
data<-ilpd[1:5,1:3]
data
is.factor(data[,1])
is.numeric(data[,2])
is.factor(data[,2])
data[,1].Primitive("abc")
length(data[,1])
length(data)
data
length(data[2,2])
data[2,2]
length(data[1:2,1:2])
data[1:2,1:2]
length(data[1:2,])
length(data[1:2,1])
length(data[1:3,1])
length(data[1:3,3])
aa <- c(123,456,789)
is.numeric(aa)
bb<-(321,654,987)
bb<-c(321,654,987)
dataFrame<-data.frame(AA=aa,BB=bb)
dataFrame
is.numeric(dataFrame[,1])
as.factor(dataFrame[,1])
is.numeric(dataFrame[0,1])
dataFrame[,1]<-as.factor(dataFrame[,1])
is.numeric(dataFrame[0,1])
is.numeric(dataFrame[1,1])
clut = read.table("file19.txt", header=T)
library(factoextra)
clut = read.table("file19.txt", header=T)
library(caret)
library(ROCR)
library(e1071)
library(arules)
library(factoextra)
clut = read.table("file19.txt", header=T)
minhash <- minhash_generator(n=160, seed=100)
library(textreuse)
r2685<-((common_movie[a[2],1]*utility_mat[a[2],9])+(common_movie[a[3],1]*utility_mat[a[3],9])+(common_movie[a[4],1]*utility_mat[a[4],9]))/(common_movie[a[2],1]+common_movie[a[3],1]+common_movie[a[4],1])
library(cluster)
library(factoextra)
library(fpc)
library(textreuse)
library(sqldf)
library(lsa)
library(magrittr)
# This is the dataset of Languages spoken in european countries.
setwd("H:\\DATA MINING\\Assignment 4")
europe = read.table("Languages Spoken in Europe.txt", header=T, skip = 17)
rownames(europe) <- europe[,1]
europe <- europe[,-1]
distMat <- dist(europe, method = 'euclidean')
distMat
#####################################################################################
# ANSWER TO QUESTION 2.1.A :
#Plotting Hierarchical Clustering using Single linkage
H_Clust_single <- eclust(europe, "hclust",  hc_method="single")
plot(H_Clust_single, labels=europe[,1])
#stats <- cluster.stats(dist(europe), H_Clust_single$cluster)
fviz_dend(H_Clust_single, data=europe, palette="uchicago", main="Single", show_labels=TRUE)
#Plotting Hierarchical clustering with Average linkage.
H_Clust_average <- eclust(europe, "hclust", hc_method="average")
plot(H_Clust_average, labels=europe[,1])
#stats <- cluster.stats(dist(europe), H_Clust_average$cluster)
fviz_dend(H_Clust_average, data=europe, palette="uchicago", main = "Average", show_labels=TRUE)
#Plotting Hierarchical Clustering with Complete linkage.
H_Clust_complete <- eclust(europe, "hclust", hc_method="complete")
plot(H_Clust_complete, labels=europe[,1])
#stats <- cluster.stats(dist(europe), H_Clust_complete$cluster)
fviz_dend(H_Clust_complete, data=europe, palette="uchicago", main="Complete", show_labels=TRUE)
######################################################################################
# ANSWER TO QUESTION 2.1B :
# All types of hierarchical clustering gives almost similar results.
# In case of Single HClustering we can see following singleton clusters:
#   i. West Germany - Austria.
#   ii. Luxemberg - Switzerland.
#   iii. France - Belgium.
#   iv. Great Britain - Ireland.
#   V. Denmark - Norway.
#In case of Average HClustering we can see following singleton clusters:
#   i. West Germany - Austria.
#   ii. Luxemberg - Switzerland.
#   iii. France - Belgium.
#   iv. Great Britain - Ireland.
#   V. Denmark - Norway.
#   vi. Portugal - Spain.
#In case of Complete HClustering we can see following singleton Clusters:
#   i. West Germany - Austria.
#   ii. Luxemberg - Switzerland.
#   iii. Great Britain - Ireland.
#   iv. Denmark - Norway.
#   v. France - Belgium.
# As we can see all three clustering share four common singleton clusters West germany - austria,
#   Luxemberg - Switzerland, Great britain - Ireland & Denmark - Norway.
################################################################################################################
# ANSWER TO QUESTION 2.1C :
# The distance between the dataset is very high with minimum value of 0 and maximum value of 100.
#    The minimum value 0 specifies that the country does'nt speak the respective language. Hence the country not
#     speaking perticular language is not an important data hence single linkage will not be best linkage.
#     Where as a coutry speaking a language is important data. Hence complete linkage would have been important.
#     But the dataset has analsis of 12 different language. Hence AVERAGE CLUSTERING will be most suitable
#     for this perticular dataset.
################################################################################################################
# ANSWER TO QUESTION 2.1D
# According to our analysis in 2.1B the most pure linkage is Average as it produces 6 singleton clusters
#   compared to other which produces 5 each.
################################################################################################################
# ANSWER TO QUESTION 2.1E
cut_tree <- cutree(H_Clust_average, h = 125)
table(cut_tree, row.names(europe))
#   At the height of 123 we have 7 Clusters:
#   i. Cluster 1: 6 Countries:  West Germany, Austria, France, Belgium, Luxemberg, Switzerland.
#   ii. Cluster 2: 1 COuntry: Italy.
#   iii. Cluster 3: 1 Country: Netherlands.
#   iv. Cluster 5 Countries: Sweden, Denmark, Norway, Great Britain, Ireland.
#   v. Cluster 6: 1 Country: Finland.
#   vi. Cluster 7: 1 Country: Spain.
##########################################################################################################
# ANSWER TO 2.1F:
#
#Plotting Hierarchical Clustering using Single linkage with 7 clusters.
H_Clust_single <- eclust(europe, "hclust",  hc_method="single", k = 7)
plot(H_Clust_single, labels=europe[,1])
stats_Single <- cluster.stats(dist(europe), H_Clust_single$cluster, silhouette = T)
fviz_dend(H_Clust_single, data=europe, palette="uchicago", main="Single", show_labels=TRUE)
#Plotting Hierarchical clustering with Average linkage with 7 clusters.
H_Clust_average <- eclust(europe, FUNcluster = "hclust", hc_method="average", k = 7)
plot(H_Clust_average, labels=europe[,1])
stats_Average <- cluster.stats(dist(europe), H_Clust_average$cluster, silhouette = T)
fviz_dend(H_Clust_average, data=europe, palette="uchicago", main = "Average", show_labels=TRUE)
#Plotting Hierarchical Clustering with Complete linkage with 7 cluster.
H_Clust_complete <- eclust(europe, "hclust", hc_method="complete", k = 7)
plot(H_Clust_complete, labels=europe[,1])
stats_Complete <- cluster.stats(dist(europe), H_Clust_complete$cluster, silhouette = T)
fviz_dend(H_Clust_complete, data=europe, palette="uchicago", main="Complete", show_labels=TRUE)
##########################################################################################################
# ANSWER FOR 2.1G:
# In cluster.stats the components which returns the dunn is dunn while the component which returns sihouette id avgsilwidth
# The DUNN for all the cluster is:
# i. Single linkage: - dunn = 0.781
stats_Single$dunn
# ii. Average linkage: - dunn = 0.807
stats_Average$dunn
# iii. Complete Linkage: - dunn = 0.677
stats_Complete$dunn
# The silhoutte width is
# i. Single Linkage - sil Width = 0.121
stats_Single$avg.silwidth
fviz_silhouette(H_Clust_single)
# ii. Average Linkage - sil width = 0.169
stats_Average$avg.silwidth
fviz_silhouette(H_Clust_average)
# iii. Complete linkage - sil width = 0.192
stats_Complete$avg.silwidth
fviz_silhouette(H_Clust_complete)
############################################################################################
# ANSWER FOR 2.1H:
# The dunn index is between 0 and infinity and has to be maximum.
#  the dunn index of Average linkage cluster is maximum of 0.807 and hence best.
##########################################################################################
# ANSWER FOR 2.1I:
##
# The Silhouette width is between -1 to 1 and has to maximum.
#  The silhouette width of complete clinkage cluster is 0.192 and hence is best.
#############################################################################################
#############################################################################################
#############################################################################################
# 2.2 LOCALITY SENSETIVE HASHING
corpus_files <- list.files("H:\\DATA MINING\\Assignment 4\\corpus", full.names=T)
minhash <- minhash_generator(n=160, seed=100)
corpus <- TextReuseCorpus(corpus_files, tokenizer = tokenize_ngrams, n = 5, minhash_func = minhash, keep_tokens = TRUE)
names(corpus)
#wordcount(corpus)
allTokens <- tokens(corpus)
length(unlist(tokens(corpus)))
#############################################################################################
#ANSWER FOR 2.2A:
# The total number of shigles in 100 documents are 22075
#############################################################################################
# ANSWER FOR 2.2B
corpusFileOnly <- list.files("H:\\DATA MINING\\Assignment 4\\corpus", full.names=F)
doc_dict <- unlist(allTokens) %>% unique()
M <- lapply(allTokens, function(set, dict) {   as.integer(dict %in% set)}, dict = doc_dict) %>% data.frame()
tempSetName <-setNames( M, paste( corpusFileOnly, 1:length(corpusFileOnly)) )
rownames(M) <- doc_dict
dim(M)
###############################################################################################
# ANSWER FOR 2.2C:
# The First 5 Shigles of document orig_taske.txt is :
tok <- tokens(corpus)
tok$orig_taske[1:5]
##############################################################################################
# ANSWER FOR 2.2D:
minhash240 <- minhash_generator(n=240, seed=100)
corpus240 <- TextReuseCorpus(corpus_files, tokenizer = tokenize_ngrams, n = 240, minhash_func = minhash240, keep_tokens = TRUE)
corpus240
length(unlist(tokens(corpus240)))
# The total number of shigles in 100 documents are 2284 with hashes = 240
# The percentage reduction in size of problem is : 89.66%
#jaccard_similarity(corpus)
##################################################################################################
# ANSWER FOR 2.2E
# The minimum number of band required is 80 to get a probablity of 0.888
lsh_probability(h = 240, b =  80, s = 0.3)
##################################################################################################
# ANSWER FOR 2.2F
# The total number of candidate pairs found is : 72
buckets <- lsh(corpus, bands = 80)
candidates <- lsh_candidates(buckets)
#candidates
#################################################################################################
# ANSWER FOR 2.2G
# The sorting the candidate pairs from highest to lowest the top 5 pairs are:
pairs <- lsh_compare(candidates, corpus, jaccard_similarity)
pairs <- pairs[order(pairs$score, decreasing = T),]
pairs[1:5,]
#pairs
##################################################################################################
# ANSWER FOR 2.2H
# If we dont use Locality Sensative Hashing and directly examined every pair for similarity then
# Number of pairs of documents to be examined  = (No of Documents)C2
# Here we can write => No of Documents = 100
# No of pairs = 100C2 = 100!/(98!*2!) = 4950
# Solution for 2.2 (h)(ii)
# No of candidate pairs generated in 2.2 (f) = 72.
# The ratio of doc pair to candidate pair number is : 4950/72 = 275/4 = 68.75
# It shows that if we dont do Locality Sensative Hashing the number of comparisons we have to do is 60.6 times than number of comparisons we will do after doing Locality Sensative Hashing.
##################################################################################################
##################################################################################################
##################################################################################################
# 2.3 Recommender Systems.
# Read the data.
setwd("H:\\DATA MINING\\Assignment 4\\ml-100k\\")
user <- read.table("u.data", fileEncoding="UTF-8", sep = "\t", col.names = c("user_id","item_id","rating","timestamp"))
item <- read.csv(file = "u.item", sep = "|", header = F)
drops <- c("V4")
item <- item[,!(names(item) %in% drops)]
colnames(item) <- c("movie_Id","movie_Title","release_Date","IMDb_url","unknown","Action","Adventure","Animation","Children's","Comedy","Crime","Documentry","Drama","Fantasy","Film-Noir","Horror","Musical","Mystery","Rommance","Sci-Fi","Thriller","War","Western")
# SQL query to retrive all movies for user id - 200
user_200 <- sqldf(x = "select * from user where user_id = '200'")
# SQL query to retrive all movies for user id - 200
movie_200 <- sqldf(x = "select * from item left join user on item.movie_id = user.item_id where user.user_id = '200'")
#Create Matrix
movie_200.matrix <- movie_200[,c(5:23)]
# generating profile vector for user_id = 200
genre_200 <- apply(X = movie_200.matrix,2,mean)
# SQL query to retrive all movies for user id - 50
user_50 <- sqldf(x = "select * from user where user_id = '50'")
# SQL query to retrive all movie for user id - 50
movieUser_50Data <- sqldf(x = "select * from item left join user on item.movie_id = user.item_id where user.user_id = '50'")
# genere matrix for user 50
movie_50.matrix <- movieUser_50Data[,c(5:23)]
# generating profile vector for user_id = 50
genre_50 <- apply(X = movie_50.matrix,2,mean)
#####################################################################################################
# ANSWER FOR QUESTION 2.3A - i:
# Cosine Similarity for user to user similarity between user 200 & user 50 : 0.54825
userSim <- cosine(x = genre_200, y = genre_50)
userSim
#####################################################################################################
# ANSWER FOR QUESTION 2.3A - ii:
# The user - item similarity  with movie id - 127 to user - 200 is : 0.55334
movie_127<- sqldf("select * from item where movie_id = '127'")
req_Movie_127 <- movie_127[,c(5:23)]
req_Movie_127 <- apply(X = req_Movie_127,2,mean)
# The vector for movie id - 127:
req_Movie_127
# User item similarity between movie 127 and user 200
u200_i127_sim <- cosine(x = genre_200, y = req_Movie_127)
u200_i127_sim
#####################################################################################################
# ANSWER FOR QUESTION 2.3A - iii:
# The user item similarity of movie with id - 127 to user - 50 is : 0.62350
u50_i127_sim <- cosine(x = genre_50, y = req_Movie_127)
u50_i127_sim
####################################################################################################
# ANSWER FOR QUESTION 2.3A - iv:
#   The similarity of movie matrix is more using genre50, this movie has high probability to be recommended by user with id = 50
###################################################################################################
# ANSWER FOR QUESTION 2.3B :
# Creating Utility Matrix:
utility_mat<-matrix(0,6,11)
for(i in 1:length(unlist(user[,1])))
{
if(user[i,1]==1 && user[i,2]<7)
{
utility_mat[user[i,2],1]<-user[i,3]
}
if(user[i,1]==21 && user[i,2]<7)
{
utility_mat[user[i,2],2]<-user[i,3]
}
if(user[i,1]==44 && user[i,2]<7)
{
utility_mat[user[i,2],3]<-user[i,3]
}
if(user[i,1]==59 && user[i,2]<7)
{
utility_mat[user[i,2],4]<-user[i,3]
}
if(user[i,1]==72 && user[i,2]<7)
{
utility_mat[user[i,2],5]<-user[i,3]
}
if(user[i,1]==82 && user[i,2]<7)
{
utility_mat[user[i,2],6]<-user[i,3]
}
if(user[i,1]==102 && user[i,2]<7)
{
utility_mat[user[i,2],7]<-user[i,3]
}
if(user[i,1]==234 && user[i,2]<7)
{
utility_mat[user[i,2],8]<-user[i,3]
}
if(user[i,1]==268 && user[i,2]<7)
{
utility_mat[user[i,2],9]<-user[i,3]
}
if(user[i,1]==409 && user[i,2]<7)
{
utility_mat[user[i,2],10]<-user[i,3]
}
if(user[i,1]==486 && user[i,2]<7)
{
utility_mat[user[i,2],11]<-user[i,3]
}
}
colnames(utility_mat)<-c("user1","user21","user44","user59","user72","user82","user102","user234","user268","user409","user486")
means <- apply(utility_mat, 1, function(x) mean(x, na.rm=T))
means
for (i in 1:dim(utility_mat)[1]) {
for (j in 1:dim(utility_mat)[2])
{
if(utility_mat[i,j]>0)
{
utility_mat[i,j] <- utility_mat[i,j] - means[i]
}
}
}
common_movie<-matrix(0,6,1)
for (i in 1:dim(utility_mat)[1])
{
common_movie[i,1]<-round(cosine(utility_mat[5,], utility_mat[i, ]), digits=2)
}
rownames(common_movie)<-c("1","2","3","4","5","6")
a<-as.numeric(rownames(common_movie)[order(common_movie, decreasing=TRUE)][1:6])
r2685<-((common_movie[a[2],1]*utility_mat[a[2],9])+(common_movie[a[3],1]*utility_mat[a[3],9])+(common_movie[a[4],1]*utility_mat[a[4],9]))/(common_movie[a[2],1]+common_movie[a[3],1]+common_movie[a[4],1])
r2685
